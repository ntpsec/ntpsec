<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="content-type" content="text/html;charset=iso-8859-1">
<meta name="generator" content="HTML Tidy, see www.w3.org">
<title>How NTP Works</title>
<link href="scripts/style.css" type="text/css" rel="stylesheet">
</head>
<body>
<h3>How NTP Works</h3>
<p>Last update:
  <!-- #BeginDate format:En2m -->11-Oct-2011  20:23<!-- #EndDate -->
  UTC</p>
<h4>Related Links</h4>
<script type="text/javascript" language="javascript" src="scripts/special.txt"></script>
<h4>Table of Contents</h4>
<ul>
  <li class="inline"><a href="#intro">Introduction and Overview</a></li>
  <li class="inline"><a href="#scale">NTP Timescale and Data Formats</a></li>
  <li class="inline"><a href="#budget">Statistics Budget</a></li>
  <li class="inline"><a href="#quality">Quality of Service</a></li>
  <li class="inline"><a href="#house">Initialization</a></li>
</ul>
<hr>
<h4 id="intro">Introduction and Overview</h4>
<blockquote>
  <p align="left">Note: This document contains a technical description of the Network Time Protocol (NTP) architecture and operation.  It is intended for  administrators, operators and monitoring personnel. Additional information for nontechnical readers can be found in the white paper <a href="http://www.eecis.udel.edu/~mills/exec.html">Executive Summary: Computer Network Time Synchronization</a>.</p>
</blockquote>
<p>NTP time synchronization services are widely available in the public Internet. The public NTP subnet in mid 2011 includes several thousand servers in most countries and on every continent of the globe, including Antarctica, and sometimes in space and on the sea floor. These servers support a total population estimated at over 25 million computers in the global Internet.</p>
<p> The NTP subnet operates with a hierarchy of levels, where each level is assigned a number called the stratum. Stratum 1 (primary) servers at the lowest level are directly synchronized to national time services via satellite, radio or telephone modem. Stratum 2 (secondary) servers at the next higher level are synchronize to stratum 1 servers and so on. Normally, NTP clients and servers with a relatively small number of clients do not synchronize to public primary servers. There are several hundred public secondary servers operating at higher strata and are the preferred choice.</p>
<p>This page presents an overview of the NTP implementation included in this software distribution. We refer to this implementation as the <em>reference implementation</em> only because it was  used to test and validate the NTPv4 specification RFC-5905. It is best read in conjunction with the briefings on the <a href="http://www.eecis.udel.edu/~mills/ntp.html">Network Time Synchronization Research Project</a> page.</p>
<div align="center">
  <p><img src="pic/fig_3_1.gif" alt="gif"></p>
  <p>Figure 1. NTP Daemon Processes and Algorithms</p>
</div>
<p>The overall organization of the NTP implementation is shown in Figure 1. It is useful in this context to consider the implementation as both a client of upstream servers and as a server for downstream clients. It includes a pair of peer/poll processes for each reference clock or remote  server used as a synchronization source. Packets are exchanged between the client and server using the <em>on-wire protocol</em> described in  the white paper NTP <a href="http://www.eecis.udel.edu/~mills/onwire.html">Analysis and Simulation of the NTP On-Wire Protocols</a>. The protocol is resistant to lost, replayed or spoofed packets.</p>
<p> The poll process sends NTP packets at intervals ranging from 8 s to 36 hr. The peer process receives NTP packets and  performs  the packet sanity tests of the <a href="decode.html#flash">flash status word</a>. The flash status word reports in addition the results of various security checks described in the white paper <a href="http://www.eecis.udel.edu/~mills/security.html">NTP Security Analysis</a>.</p>
<p>Packets that fail one or more of these tests are summarily discarded. Otherwise, the peer process runs the on-wire protocol that uses four raw timestamps: the <em>origin timestamp</em> <em>T</em><sub>1</sub> upon departure of the client request, the <em>receive timestamp</em> <em>T</em><sub>2</sub> upon arrival at the server, the <em>transmit timestamp</em> <em>T</em><sub>3</sub> upon departure of the server reply, and the <em>destination timestamp</em> <em>T</em><sub>4</sub> upon arrival at the client. These timestamps, which are recorded by the <tt>rawstats</tt> option of the <a href="monopt.html#filegen"><tt>filegen</tt></a> command, are used to calculate the clock offset and roundtrip delay samples:</p>
<div align="center">
  <p>offset = [(<em>T</em><sub>2</sub> -<em> T</em><sub>1</sub>) + (<em>T</em><sub>3</sub> - <em>T</em><sub>4</sub>)] / 2,<br>
    delay = (<em>T</em><sub>4</sub> - <em>T</em><sub>1</sub>) - (<em>T</em><sub>3</sub> - <em>T</em><sub>2</sub>).</p>
</div>
<p>The  algorithm described on the <a href="filter.html">Clock Filter Algorithm</a> page  selects the    offset  and  delay samples most likely to produce accurate results. Those servers that have passed  the  sanity tests     are declared <em>selectable</em>. From the selectable population the statistics are used by the algorithm described on the <a href="select.html">Clock Select Algorithm</a> page to determine a number of <em>truechimers</em> according to correctness principles. From the truechimer population the algorithm described on the <a href="cluster.html">Clock Cluster Algorithm</a> page determines a number of <em>survivors</em> on the basis of statistical clustering principles. The algorithms described on the <a href="prefer.html">Mitigation Rules and the <tt>prefer</tt> Keyword</a> page combine the survivor offsets, designate one of them as the <em>system peer</em> and produces the final offset used by the algorithm described on the <a href="discipline.html">Clock Discipline Algorithm</a> page to adjust the system clock time and frequency. The clock offset and frequency, are recorded by the <tt>loopstats</tt> option of the <a href="monopt.html#filegen"><tt>filegen</tt></a> command. For additional details about these algorithms, see the Architecture Briefing on the <a href="http://www.eecis.udel.edu/~mills/ntp.html">Network Time Synchronization Research Project</a> page.</p>
<h4 id="scale">NTP Timescale and Data Formats</h4>
<p>NTP clients and servers synchronize to the  Coordinated Universal Time (UTC) timescale used by national laboratories and disseminated by radio, satellite and telephone modem. This is a global timescale independent of geographic position. There are no provisions for local time zone or daylight savings time; however, these functions can be performed by the operating system on a per-user basis.</p>
<p> The UT1 timescale, upon which UTC is based, is determined by the rotation of the Earth about its axis, which is gradually slowing down. In order to rationalize  UTC with respect to UT1, a  leap second is inserted  at intervals of about 18 months, as determined by the  International Earth Rotation Service (IERS). The historic insertions are documented in the <tt>leap-seconds.list</tt> file, which can be downloaded from the NIST FTP server. This file is updated at intervals not exceeding six months. Leap second warnings are disseminated by the national laboratories in the broadcast timecode format. These warnings are propagated from the NTP primary servers via other server to the clients  by the NTP on-wire protocol. The leap second is implemented by the operating system kernel, as described in  the white  paper <a href="http://www.eecis.udel.edu/~mills/leap.html">The NTP Timescale and Leap Seconds</a>.</p>
<p>There are two NTP time formats, a 64-bit <em>timestamp</em> format and a `128-bit <em>date</em> format. The date format is used internally, while the timestamp format is used in packet headers exchanged between clients and servers. The timestamp format spans 136 years, called an <em>era</em>. The current era began on 1 January 1900, while  the next one begins in 2036. Details on these formats and conversions between them are  in the white paper <a href="http://www.eecis.udel.edu/~mills/y2k.html">The NTP Era and Era Numbering</a>. However, the NTP protocol will synchronize correctly, regardless of era, as long as  the system clock is set initially within  68 years of the correct time. Further discussion on this issue is in the white paper <a href="http://www.eecis.udel.edu/~mills/time.html">NTP Timestamp Calculations</a>. Ordinarily, these formats are not seen by application programs, which convert these NTP formats to native Unix or Windows formats.</p>
<h4 id="budget">Statistics Budget</h4>
<p>Each NTP synchronization source is characterized by the   offset and delay   samples measured by the on-wire protocol using the equations above. The  dispersion  sample is initialized with the precision as each sample is received and increments at a rate of 15 <font face="symbol">m</font>s/s after that. These are called the sample statistics (<em>offset, delay, dispersion).</em></p>
<p> In a window of eight (offset, delay, dispersion) samples, the clock filter algorithm selects the  sample with  minimum  delay, which generally represents the most accurate offset statistic. The selected sample becomes the <em>peer offset</em> and <em>peer delay </em>statistics. The <em>peer dispersion</em> is  a weighted average of the dispersion samples in the window.  It is recalculated as each sample update is received from the server. Between updates, the dispersion continues to grow at the same  rate as the sample dispersion, 15 <font face="symbol">m</font>s/s. Finally, the <em>peer jitter</em> is determined as the root-mean-square (RMS) average of  the offset samples in the window relative to the selected offset sample. The peer offset, peer delay, peer dispersion and peer jitter  statistics are recorded by the <tt>peerstats</tt> option of the <a href="monopt.html#filegen"><tt>filegen</tt></a> command. Peer variables are displayed by the <tt>rv</tt> command of the <a href="ntpq.html#peer"><tt>ntpq</tt></a> program.</p>
<p> The clock filter algorithm continues to process packets in this way until the source is no longer reachable. Reachability is determined by an eight-bit shift register, which is shifted left by one bit as each poll packet is sent, with 0 replacing the vacated rightmost bit. Each time an update is received, the rightmost bit is set to 1. The source is considered reachable if any bit is set to 1 in the register; otherwise, it is considered unreachable.</p>
<p>A server is considered selectable only if it is reachable, the dispersion is below the select threshold and a timing loop would not be created. A timing loop occurs when the server is apparently synchronized to the client or when the server is synchronized to the same server as the client. When a source is unreachable, a  dummy sample with  &quot;infinite&quot; dispersion  is inserted in the shift register at each poll, thus displacing old samples.</p>
<p>The composition of  the survivor population and the system peer selection is redetermined as each update from each source is received. The system variables are copied from the  system peer variables of the same name and the system stratum set one greater than the system peer stratum. System variables are displayed by the <tt>rv</tt> command of the <a href="ntpq.html#system"><tt>ntpq</tt></a> program.</p>
<p> Like  peer dispersion, the system dispersion increases at the same rate so, even if all sources have become unreachable, the server appears to dependent clients at ever increasing dispersion. If the system dispersion exceeds the select threshold, as apparent to dependent clients, the server is considered unselectable. It is important to understand that a server in this condition remains a reliable source of synchronization within its error bounds, as described in the next section.</p>
<h4 id="quality">Quality of Service</h4>
<p>The mitigation algorithms deliver several important statistics, including <em>system offset</em> and <em>system jitter</em>. These statistics are determined by the mitigation algorithms's from the survivor statistics produced by the clock cluster algorithm. System offset is best interpreted as the maximum likelihood estimate of the system clock offset, while system jitter is best interpreted as the expected error of this estimate. These statistics are reported by the <tt>loopstats</tt> option of the <a href="monopt.html#filegen"><tt>filegen</tt></a> command.</p>
<p>Of interest in this discussion is how the client determines the quality of service from a particular reference clock or remote server. This is determined from two statistics, <em>expected error</em> and <em>maximum error.</em> Expected error, or  system jitter, is determined from various jitter components; it represents the nominal error in determining the mean clock offset. </p>
<p>Maximum error is determined from delay and dispersion contributions and represents the worst-case error due to all causes. In order to simplify discussion, certain minor contributions to the maximum error statistic are ignored. Elsewhere in the documentation the maximum error is called <em>synchronization distance</em>. If the precision time kernel support is available, both the estimated error and maximum error are reported to user programs via the <tt>ntp_gettime()</tt> kernel system call. See the <a href="kern.html">Kernel Model for Precision Timekeeping</a> page for further information.</p>
<p>The maximum error  is  computed as one-half the <em>root delay</em> to the primary source of time; i.e., the primary reference clock, plus the <em>root dispersion</em>.   The root variables are included in the NTP packet header received from each server. When calculating  maximum error, the  root delay is the sum of the root delay in the packet and the peer  delay, while the root dispersion is the sum of the root dispersion in the packet and the peer dispersion.</p>
<p>A source is considered selectable only if its maximum error is less than the <em>select threshold</em>, by default 1.5 s, but can be changed according to client preference using the <tt>maxdist</tt> option of the <a href="miscopt.html#tos"><tt>tos</tt></a> command. A common  consequences is when an upstream server loses all sources and its maximum error apparent to dependent clients begins to increase. The clients are not aware of  this condition and  continues to accept synchronization as long as the maximum error is less than the select threshold.</p>
<p>Although it might seem counter-intuitive, a cardinal rule in the selection process is, once a sample has been selected by the clock filter algorithm,  older samples are no longer selectable. This applies also to the clock select algorithm. Once the peer variables for a source have been selected, older variables of the same or other sources are no longer selectable. The reason for these rules is to limit the time delay in the the clock discipline algorithm. This is necessary to  preserve the optimum impulse response and thus the risetime and overshoot.</p>
<p>This means that not every sample can be used to update the peer variables and up to seven samples can be ignored between selected samples. This fact has been carefully considered in the discipline algorithm design with due consideration for feedback loop delay and minimum sampling rate. In engineering terms, even if only one sample in eight survives, the resulting sample rate is twice the Nyquist rate at any time constant and poll interval.</p>
<h4 id="house">Initialization</h4>
<p>If left running continuously, an NTP client on a fast LAN in a home or office environment can  maintain synchronization  nominally within one millisecond. When the ambient temperature variations are less than a degree Celsius,  the clock oscillator frequency is  disciplined to within one part per million (PPM), even when the   undisciplined frequency error is 100 PPM or more. However, when the client is  restarted after a period when the power is off, there clock  may  have significant error. The provisions described in this section insure that, in all but pathological situations, the transient is suppressed to within nominal levels in no more than five minutes after a warm start or ten minutes after a cold start.</p>
<p> For laptops and portable devices when the power is turned off,  the   battery backup clock offset error can increase   as much as one second  per day. When  power is restored after  several hours or days,  the  clock offset and oscillator frequency   errors must be resolved by the clock discipline algorithm, but this can take several hours without specific provisions. The reference implementation   records the oscillator frequency  in a file, which is updated at intervals of one hour or more, depending on the measured frequency wander. This design is intended to minimize  write cycles in NVRAM that might  be used in a laptop or portable device.</p>
<p> In a warm start, the frequency is initialized from the frequency file, which avoids a possibly lengthy discipline time. In a cold start when no frequency file is available, the reference implementation measures the   oscillator frequency  over a five-minute interval. This generally results in a residual frequency error of less than  1 PPM. The measurement interval can be changed using the <tt>stepout</tt> option of the <a href="miscopt.html#tinker"><tt>tinker</tt></a> command.</p>
<p>In order to further reduce the  clock offset error at at restart, the reference implementation  first disables oscillator frequency discipline and enables clock offset discipline with a small time constant. This  is designed to quickly reduce the clock offset error without causing a frequency surge. This configuration is continued for an interval of five minutes, after which the  clock offset error is usually  no more than a millisecond. The  measurement interval can be changed using the <tt>stepout</tt> option of the <a href="miscopt.html#tinker"><tt>tinker</tt></a> command.</p>
<p>Another  concern at  restart is the  time necessary for  the selection and clustering algorithms to refine and validate the initial  clock offset estimate. Normally, this takes several updates before setting the system clock. As the default minimum poll interval in most configurations is about one minute, it can take several minutes before setting the system clock. The <tt>iburst</tt> option of the <a href="confopt.html#burst"><tt>server</tt></a> command  changes the behavior at restart and is    recommended for client/server configurations.  When this option is enabled, the client  sends a volley of six requests at intervals of two seconds. This insures a reliable estimate is available in about ten seconds before setting the clock. Once this initial volley is complete, the procedures described above are executed.</p>
<p>As a result of the above considerations, when a backup source, such as the local clock driver, ACTS modem driver or orphan mode is included in the system configuration, it may happen that one or more of them are selectable before one or more of the  regular sources are selectable. When backup sources are included in the configuration, the  reference implementation waits an interval of several minutes without regular sources before switching to backup sources. This is generally enough to avoid startup transients due to premature switching to backup sources. The interval can be changed using the <tt>orphanwait</tt> option of the <a href="miscopt.html#tos"><tt>tos</tt></a> command.</p>
<hr>
<script type="text/javascript" language="javascript" src="scripts/footer.txt"></script>
</body>
</html>
