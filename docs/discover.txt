= Automatic Server Discovery Schemes =

[cols="10%,90%",frame="none",grid="none",style="verse"]
|==============================
|image:pic/alice51.gif[]|
http://www.eecis.udel.edu/%7emills/pictures.html[from 'Alice's Adventures in Wonderland', Lewis Carroll]

Make sure who your friends are.

|==============================

== Related Links ==

include::includes/hand.txt[]

== Table of Contents ==

* link:#assoc[Association Management]
* link:#bcst[Broadcast/Multicast Scheme]
* link:#mcst[Manycast Scheme]
* link:#pool[Server Pool Scheme]

'''''

[[modes]]
== Introduction ==

This page describes the automatic server discovery schemes provided in
NTPv4. There are three automatic server discovery schemes:
broadcast/multicast, many cast, and server pool, which are described on
this page. The broadcast/multicast and many cast schemes utilize the
ubiquitous broadcast or one-to-many paradigm native to IPv4 and IPv6.
The server pool scheme uses DNS to resolve addresses of multiple
volunteer servers scattered throughout the world.

All three schemes work in much the same way and might be described as
_grab-n'-prune._ Through one means or another they grab a number of
associations either directly or indirectly from the configuration file,
order them from best to worst according to the NTP mitigation
algorithms, and prune the surplus associations.

[[assoc]]
== Association Management ==

All schemes use an iterated process to discover new preemptable client
associations as long as the total number of client associations is less
than the +maxclock+ option of the +tos+ command. The +maxclock+ default
is 10, but it should be changed in typical configuration to some lower
number, usually two greater than the +minclock+ option of the same
command.

All schemes use a stratum filter to select just those servers with
stratum considered useful. This can avoid large numbers of clients
ganging up on a small number of low-stratum servers and avoid servers
below or above specified stratum levels. By default, servers of all
strata are acceptable; however, the +tos+ command can be used to
restrict the acceptable range from the +floor+ option, inclusive, to the
+ceiling+ option, exclusive. Potential servers operating at the same
stratum as the client will be avoided, unless the +cohort+ option is
present. Additional filters can be supplied using the methods described
on the link:authentic.html[Authentication Support] page.

The pruning process uses a set of unreach counters, one for each
association created by the configuration or discovery processes. At each
poll interval, the counter is increased by one. If an acceptable packet
arrives for a persistent (configured) or ephemeral (broadcast/multicast)
association, the counter is set to zero. If an acceptable packet arrives
for a preemptable (manycast, pool) association and survives the
selection and clustering algorithms, the counter is set to zero. If the
the counter reaches an arbitrary threshold of 10, the association
becomes a candidate for pruning.

The pruning algorithm is very simple. If an ephemeral or preemptable
association becomes a candidate for pruning, it is immediately
demobilized. If a persistent association becomes a candidate for
pruning, it is not demobilized, but its poll interval is set at the
maximum. The pruning algorithm design avoids needless discovery/prune
cycles for associations that wander in and out of the survivor list, but
otherwise have similar characteristics.

Following is a summary of each scheme. Note that reference to option
applies to the commands described on the link:confopt.html[Configuration
Options] page. See that page for applicability and defaults.

[[bcst]]
== Broadcast/Multicast Scheme ==

A broadcast server generates messages continuously at intervals by
default 64 s and time-to-live by default 127. These defaults can be
overridden by the +minpoll+ and +ttl+ options, respectively. Not all
kernels support the +ttl+ option. A broadcast client responds to the
first message received by waiting a randomized interval to avoid
implosion at the server. It then polls the server in client/server mode
using the +iburst+ option in order to quickly authenticate the server,
calibrate the propagation delay and set the client clock. This normally
results in a volley of six client/server exchanges at 2-s intervals
during which both the synchronization and cryptographic protocols run
concurrently.

Following the volley, the server continues in listen-only mode and sends
no further messages. If for some reason the broadcast server does not
respond to these messages, the client will cease transmission and
continue in listen-only mode with a default propagation delay. The
volley can be avoided by using the +broadcastdelay+ command with nonzero
argument.

A server is configured in broadcast mode using the +broadcast+ command
and specifying the broadcast address of a local interface. If two or
more local interfaces are installed with different broadcast addresses,
a +broadcast+ command is needed for each address. This provides a way to
limit exposure in a firewall, for example. A broadcast client is
configured using the +broadcastclient+ command.

NTP multicast mode can be used to extend the scope using IPv4 multicast
or IPv6 broadcast with defined span. The IANA has assigned IPv4
multicast address 224.0.1.1 and IPv6 address FF05::101 (site local) to
NTP, but these addresses should be used only where the multicast span
can be reliably constrained to protect neighbor networks. In general,
administratively scoped IPv4 group addresses should be used, as
described in RFC-2365, or GLOP group addresses, as described in
RFC-2770.

A multicast server is configured using the +broadcast+ command, but
specifying a multicast address instead of a broadcast address. A
multicast client is configured using the +multicastclient+ command
specifying a list of one or more multicast addresses. Note that there is
a subtle distinction between the IPv4 and IPv6 address families. The
IPv4 broadcast or mulitcast mode is determined by the IPv4 class. For
IPv6 the same distinction can be made using the link-local prefix FF02
for each interface and site-local prefix FF05 for all interfaces.

It is possible and frequently useful to configure a host as both
broadcast client and broadcast server. A number of hosts configured this
way and sharing a common broadcast address will automatically organize
themselves in an optimum configuration based on stratum and
synchronization distance.

Since an intruder can impersonate a broadcast server and inject false
time values, broadcast mode should always be cryptographically
authenticated. By default, a broadcast association will not be mobilized
unless cryptographically authenticated. If necessary, the +auth+ option
of the +disable+ command will disable this feature. The feature can be
selectively enabled using the +notrust+ option of the +restrict+
command.

With symmetric key cryptography each broadcast server can use the same
or different keys. In one scenario on a broadcast LAN, a set of
broadcast clients and servers share the same key along with another set
that share a different key. Only the clients with matching key will
respond to a server broadcast. Further information is on the
link:authentic.html[Authentication Support] page.

[[mcst]]
== Manycast Scheme ==

Manycast is an automatic server discovery and configuration paradigm.
It is intended as a means for a client to troll the nearby network
neighborhood to find cooperating servers, validate them using
cryptographic means and evaluate their time values with respect to
other servers that might be lurking in the vicinity. It uses the
grab-n'-drop paradigm with the additional feature that active means
are used to grab additional servers should the number of associations
fall below the +maxclock+ option of the +tos+ command. The intended
result is that each manycast client mobilizes client associations with
some number of the "best" of the nearby manycast servers, yet
automatically reconfigures to sustain this number of servers should
one or another fail.

The manycast paradigm is not the anycast paradigm described in RFC-1546,
which is designed to find a single server from a clique of servers
providing the same service. The manycast paradigm is designed to find a
plurality of redundant servers satisfying defined optimality criteria.

Manycasting can be used with symmetric-key cryptography.

A manycast client is configured using the +manycastclient+
configuration command, which is similar to the +server+ configuration
command but with a multicast (IPv4 class _D_ or IPv6 prefix _FF_)
group address. The IANA has designated IPv4 address 224.1.1.1 and IPv6
address FF05::101 (site local) for NTP.

The client sends ordinary client mode messages, but to one of
these broadcast addresses rather than a unicast address, and sends
only if less than +maxclock+ associations remain and then only at the
minimum feasible rate and minimum feasible time-to-live (TTL)
hops. The polling strategy is designed to reduce as much as possible
the volume of broadcast messages and the effects of implosion due to
near-simultaneous arrival of manycast server messages. There can be as
many manycast client associations as different addresses, each one
serving as a template for future unicast client/server associations.

A manycast server is configured using the +manycastserver+ command,
which listens on the specified broadcast address for manycast client
messages.  If a manycast server is in scope of the current TTL and is
itself synchronized to a valid source and operating at a stratum level
equal to or lower than the manycast client, it replies with an ordinary
unicast server message.

The manycast client receiving this message mobilizes a preemptable
client association according to the matching manycast client template.
This requires the server to be cryptographically authenticated and the
server stratum to be less than or equal to the client stratum.

Then, the client polls the server at its unicast address in
burst mode in order to reliably set the host clock and validate the
source. This normally results in a volley of eight client/server at 2-s
intervals during which both the synchronization and cryptographic
protocols run concurrently. Following the volley, the client runs the
NTP intersection and clustering algorithms, which act to discard all but
the "best" associations according to stratum and synchronization
distance. The surviving associations then continue in ordinary
client/server mode.

The manycast client polling strategy is designed to reduce as much as
possible the volume of manycast client messages and the effects of
implosion due to near-simultaneous arrival of manycast server messages.
The strategy is determined by the _manycastclient_, _tos_ and _ttl_
configuration commands. The manycast poll interval is normally eight
times the system poll interval, which starts out at the _minpoll_ value
specified in the _manycastclient_, command and, under normal
circumstances, increments to the _maxpolll_ value specified in this
command. Initially, the TTL is set at the minimum hops specified by the
ttl command. At each retransmission the TTL is increased until reaching
the maximum hops specified by this command or a sufficient number of client
associations have been found. Further retransmissions use the same TTL.

The quality and reliability of the suite of associations discovered by
the manycast client is determined by the NTP mitigation algorithms and
the _minclock_ and _minsane_ values specified in the +tos+ configuration
command. At least _minsane_ candidate servers must be available and the
mitigation algorithms produce at least _minclock_ survivors in order to
synchronize the clock. Byzantine agreement principles require at least
four candidates in order to correctly discard a single falseticker. For
legacy purposes, _minsane_ defaults to 1 and _minclock_ defaults to 3.
For manycast service _minsane_ should be explicitly set to 4, assuming
at least that number of servers are available.

If at least _minclock_ servers are found, the manycast poll interval is
immediately set to eight times _maxpoll_. If less than _minclock_
servers are found when the TTL has reached the maximum hops, the
manycast poll interval is doubled. For each transmission after that, the
poll interval is doubled again until reaching the maximum of eight times
_maxpoll_. Further transmissions use the same poll interval and TTL
values. Note that while all this is going on, each client/server
association found is operating normally at the system poll interval.

Administratively scoped multicast boundaries are normally specified by
the network router configuration and, in the case of IPv6, the link/site
scope prefix. By default, the increment for TTL hops is 32 starting from
31; however, the _ttl_ configuration command can be used to modify the
values to match the scope rules.

It is often useful to narrow the range of acceptable servers which can
be found by manycast client associations. Because manycast servers
respond only when the client stratum is equal to or greater than the
server stratum, primary (stratum 1) servers will find only primary
servers in TTL range, which is probably the most common objective.
However, unless configured otherwise, all manycast clients in TTL range
will eventually find all primary servers in TTL range, which is probably
not the most common objective in large networks. The +tos+ command can
be used to modify this behavior. Servers with stratum below _floor_ or
above _ceiling_ specified in the +tos+ command are strongly discouraged
during the selection process; however, these servers may be temporally
accepted if the number of servers within TTL range is less than
_minclock_.

The above actions occur for each manycast client message, which repeats
at the designated poll interval. However, once the ephemeral client
association is mobilized, subsequent manycast server replies are
discarded, since that would result in a duplicate association. If during
a poll interval the number of client associations falls below
_minclock_, all manycast client prototype associations are reset to the
initial poll interval and TTL hops and operation resumes from the
beginning. It is important to avoid frequent manycast client messages,
since each one requires all manycast servers in TTL range to respond.
The result could well be an implosion, either minor or major, depending
on the number of servers in range. The recommended value for _maxpoll_
is 12 (4,096 s).

It is possible and frequently useful to configure a host as both
manycast client and manycast server. A number of hosts configured this
way and sharing a common multicast group address will automatically
organize themselves in an optimum configuration based on stratum and
synchronization distance. For example, consider an NTP subnet of two
primary servers and a hundred or more dependent clients. With two
exceptions, all servers and clients have identical configuration files
including both +multicastclient+ and +multicastserver+ commands using,
for instance, multicast group address 239.1.1.1. The only exception is
that each primary server configuration file must include commands for
the primary reference source such as a GPS receiver.

The remaining configuration files for all secondary servers and clients
have the same contents, except for the +tos+ command, which is specific
for each stratum level. For stratum 1 and stratum 2 servers, that
command is not necessary. For stratum 3 and above servers the _floor_
value is set to the intended stratum number. Thus, all stratum 3
configuration files are identical, all stratum 4 files are identical and
so forth.

Once operations have stabilized in this scenario, the primary servers
will find the primary reference source and each other, since they both
operate at the same stratum (1), but not with any secondary server or
client, since these operate at a higher stratum. The secondary servers
will find the servers at the same stratum level. If one of the primary
servers loses its GPS receiver, it will continue to operate as a client
and other clients will time out the corresponding association and
re-associate accordingly.

Some administrators prefer to avoid running {ntpdman} continuously and
run {ntpdman} +-q+ as a cron job. In either case the servers must be
configured in advance and the program fails if none are available when
the cron job runs. A really slick application of manycast is with
{ntpd} +-q+. The program wakes up, scans the local landscape looking
for the usual suspects, selects the best from among the rascals, sets
the clock and then departs. Servers do not have to be configured in
advance and all clients throughout the network can have the same
configuration file.

The use of cryptographic authentication is always a good idea in any
server discovery scheme. Both symmetric key and public key cryptography
can be used in the same scenarios as described above for the
broadcast/multicast scheme.

[[pool]]
== Server Pool Scheme ==

The idea of targeting servers on a random basis to distribute and
balance the load is not a new one; however, the NTP pool scheme puts
this on steroids. At present, several thousand operators around the
globe have volunteered their servers for public access. In general,
NTP is a lightweight service and servers used for other purposes don't
mind an additional small load. The trick is to randomize over the
population and minimize the load on any one server while retaining the
advantages of multiple servers using the NTP mitigation algorithms.

To support this service, custom DNS software is used by pool.ntp.org and
its subdomains to discover a random selection of participating servers
in response to a DNS query. The client receiving this list mobilizes
some or all of them, similar to the manycast discovery scheme, and
prunes the excess. Unlike +manycastclient+, cryptographic authentication
is not required. The pool scheme solicits a single server at a time,
compared to +manycastclient+ which solicits all servers within a
multicast TTL range simultaneously. Otherwise, the pool server discovery
scheme operates as manycast does.

The pool scheme is configured using one or more +pool+ commands with DNS
names indicating the pool from which to draw. The +pool+ command can be
used more than once; duplicate servers are detected and discarded. In
principle, it is possible to use a configuration file containing a
single line +pool   pool.ntp.org+. The
http://www.pool.ntp.org/en/use.html[NTP Pool Project] offers
instructions on using the pool with the +server+ command, which is
suboptimal but works with older versions of +{ntpd}+ predating the +pool+
command. Consider replacing the multiple +server+
commands in their example with a single +pool+ command.

'''''

include::includes/footer.txt[]
